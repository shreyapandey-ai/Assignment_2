# ETL Pipeline â€“ JSONPlaceholder Users API

## ğŸ“Œ Project Overview

This project implements a **clean and modular ETL (Extract, Transform, Load) pipeline** using Python.  
The pipeline fetches user data from a public REST API, validates and cleans the data, and stores the processed output for further analysis.

The project focuses on **core data engineering fundamentals**:
- API data extraction
- Data validation
- Modular pipeline design
- Clean separation of concerns

this is the user db


## ğŸ—ï¸ Pipeline Architecture

JSONPlaceholder API
â†“
Extract (API Fetch)
â†“
Validate (Data Quality Rules)
â†“
Transform (Clean Structure)
â†“
Load (CSV / Local Storage)

yaml
Copy code

---

## ğŸ”— Data Source

- **API**: https://jsonplaceholder.typicode.com/users  
- **Format**: Nested JSON  
- **Records**: 10 demo users  
- **Note**: This is a public mock API used for learning and testing

---

## ğŸ“‚ Project Structure

Assignment_2/
â”‚
â”œâ”€â”€ pipeline.py # Pipeline orchestrator
â”œâ”€â”€ extract.py # API extraction logic
â”œâ”€â”€ validate.py # Data validation rules
â”œâ”€â”€ transform.py # Data cleaning & restructuring
â”œâ”€â”€ data/ # Output files (CSV)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

yaml
Copy code

---

## âš™ï¸ Technologies Used

- **Python 3**
- **Requests** â€“ API communication
- **CSV / File System** â€“ Data storage
- **Virtual Environment** â€“ Dependency isolation

---

## âœ… Data Validation Rules

The pipeline validates each user record before further processing.

| Rule | Action |
|-----|-------|
| Missing required fields | Reject record |
| Invalid email format | Reject record |
| Empty name or username | Reject record |
| Invalid nested address data | Reject record |

Invalid records are **excluded**, ensuring only clean data is stored.

---

## ğŸ”„ Transformation Highlights

- Flattens nested JSON structures (address, company)
- Extracts only relevant fields
- Produces a clean, tabular structure suitable for analytics
- Removes unused or inconsistent attributes

---

## ğŸ“ Output

- **Cleaned CSV file** containing validated user data
- File is generated automatically when the pipeline runs

Example output fields:
- user_id
- name
- username
- email
- city
- company_name

this is the user data
![User Data](images/insights.png)

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/shreyapandey-ai/Assignment_2.git
cd Assignment_2
2ï¸âƒ£ Create Virtual Environment
bash
Copy code
python3 -m venv venv
source venv/bin/activate
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install requests
4ï¸âƒ£ Run the Pipeline
bash
Copy code
python3 pipeline.py
ğŸ§ª Sample Console Output
yaml
Copy code
Fetching data from API...
Total records fetched: 10
Valid records: 10
Invalid records: 0
Pipeline execution completed successfully.
ğŸ§  Key Design Decisions
Modular ETL structure (Extract / Validate / Transform)

Fail-fast approach if API fetch fails

Explicit validation instead of trusting API data

Easy extensibility for database or analytics integration

ğŸ¯ Use Cases
ETL pipeline demonstration

Data engineering fundamentals

API data ingestion practice

Portfolio project

ğŸ“Œ Notes
Uses a public demo API (data is synthetic)

Designed for clarity and correctness

Can be extended to include SQLite, Pandas, and SQL analytics

ğŸ‘¤ Author
Shreya Pandey
GitHub: https://github.com/shreyapandey-ai

ğŸ“ License
This project is for educational and assignment purposes only.
